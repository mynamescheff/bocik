{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, image_files, annotations, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.image_files = image_files\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.data_dir, 'images', self.image_files[idx])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        filename = self.image_files[idx]\n",
    "        annotation = self.annotations.get(filename, {})\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeWithAnnotations(T.Compose):\n",
    "    def __call__(self, image, annotation):\n",
    "        image = self.transforms[0](image)\n",
    "\n",
    "        original_width, original_height = image.size\n",
    "        bbox_coords = annotation[\"bbox\"]\n",
    "        scale_factor_x = image.width / original_width\n",
    "        scale_factor_y = image.height / original_height\n",
    "        new_bbox_coords = [coord * scale_factor_x if idx % 2 == 0 else coord * scale_factor_y for idx, coord in enumerate(bbox_coords)]\n",
    "        annotation[\"bbox\"] = new_bbox_coords\n",
    "\n",
    "        return image, annotation\n",
    "    \n",
    "# Define the transformations for the dataset\n",
    "transform = ResizeWithAnnotations([\n",
    "    T.Resize((800, 800)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_dict = {\n",
    "    \"Screenshot_1.png\": [912, 411, 1022, 543],\n",
    "    \"Screenshot_10.png\": [180, 142, 408, 383],\n",
    "    \"Screenshot_100.png\": [202, 226, 300, 348],\n",
    "    \"Screenshot_101.png\": [266, 159, 369, 285],\n",
    "    \"Screenshot_102.png\": [334, 275, 431, 411],\n",
    "    \"Screenshot_103.png\": [386, 314, 501, 474],\n",
    "    \"Screenshot_104.png\": [707, 373, 796, 522],\n",
    "    \"Screenshot_105.png\": [852, 201, 950, 315],\n",
    "    \"Screenshot_106.png\": [669, 258, 718, 330],\n",
    "    \"Screenshot_108.png\": [534, 193, 626, 298],\n",
    "    \"Screenshot_109.png\": [476, 205, 569, 321],\n",
    "    \"Screenshot_11.png\": [185, 140, 1051, 756],\n",
    "    \"Screenshot_110.png\": [255, 361, 360, 475],\n",
    "    \"Screenshot_111.png\": [320, 373, 412, 479],\n",
    "    \"Screenshot_112.png\": [406, 415, 503, 532],\n",
    "    \"Screenshot_12.png\": [671, 284, 1036, 530],\n",
    "    \"Screenshot_13.png\": [514, 113, 614, 523],\n",
    "    \"Screenshot_14.png\": [421, 628, 584, 781],\n",
    "    \"Screenshot_15.png\": [862, 270, 932, 354],\n",
    "    \"Screenshot_16.png\": [112, 334, 211, 462],\n",
    "    \"Screenshot_17.png\": [777, 95, 834, 174],\n",
    "    \"Screenshot_18.png\": [581, 133, 632, 213],\n",
    "    \"Screenshot_19.png\": [667, 77, 742, 143],\n",
    "    \"Screenshot_2.png\": [798, 475, 871, 579],\n",
    "    \"Screenshot_20.png\": [688, 424, 769, 511],\n",
    "    \"Screenshot_21.png\": [755, 187, 810, 241],\n",
    "    \"Screenshot_22.png\": [524, 171, 570, 221],\n",
    "    \"Screenshot_23.png\": [405, 416, 472, 486],\n",
    "    \"Screenshot_24.png\": [459, 211, 515, 284],\n",
    "    \"Screenshot_25.png\": [1415, 397, 1475, 475],\n",
    "    \"Screenshot_26.png\": [521, 290, 647, 468],\n",
    "    \"Screenshot_27.png\": [523, 396, 625, 496],\n",
    "    \"Screenshot_28.png\": [185, 198, 1268, 443],\n",
    "    \"Screenshot_29.png\": [345, 104, 593, 433],\n",
    "    \"Screenshot_3.png\": [288, 336, 406, 475],\n",
    "    \"Screenshot_30.png\": [140, 145, 1797, 337],\n",
    "    \"Screenshot_31.png\": [339, 252, 428, 352],\n",
    "    \"Screenshot_32.png\": [922, 218, 999, 316],\n",
    "    \"Screenshot_33.png\": [1138, 374, 1233, 490],\n",
    "    \"Screenshot_34.png\": [1176, 263, 1235, 328],\n",
    "    \"Screenshot_35.png\": [161, 198, 229, 260],\n",
    "    \"Screenshot_36.png\": [197, 128, 258, 182],\n",
    "    \"Screenshot_37.png\": [671, 172, 729, 231],\n",
    "    \"Screenshot_38.png\": [314, 183, 1460, 382],\n",
    "    \"Screenshot_39.png\": [226, 180, 1029, 705],\n",
    "    \"Screenshot_4.png\": [383, 326, 447, 410],\n",
    "    \"Screenshot_40.png\": [285, 238, 359, 307],\n",
    "    \"Screenshot_41.png\": [328, 293, 412, 378],\n",
    "    \"Screenshot_42.png\": [991, 375, 1092, 481],\n",
    "    \"Screenshot_43.png\": [935, 217, 1003, 281],\n",
    "    \"Screenshot_44.png\": [266, 143, 343, 198],\n",
    "    \"Screenshot_45.png\": [483, 264, 550, 335],\n",
    "    \"Screenshot_46.png\": [885, 255, 942, 314],\n",
    "    \"Screenshot_47.png\": [1226, 125, 1282, 188],\n",
    "    \"Screenshot_48.png\": [643, 417, 707, 499],\n",
    "    \"Screenshot_49.png\": [421, 269, 479, 332],\n",
    "    \"Screenshot_5.png\": [72, 301, 155, 405],\n",
    "    \"Screenshot_50.png\": [604, 233, 666, 291],\n",
    "    \"Screenshot_51.png\": [1305, 244, 1372, 317],\n",
    "    \"Screenshot_52.png\": [543, 97, 588, 161],\n",
    "    \"Screenshot_53.png\": [787, 324, 845, 414],\n",
    "    \"Screenshot_54.png\": [613, 288, 737, 376],\n",
    "    \"Screenshot_55.png\": [585, 359, 684, 481],\n",
    "    \"Screenshot_56.png\": [977, 213, 1015, 271],\n",
    "    \"Screenshot_57.png\": [215, 402, 254, 461],\n",
    "    \"Screenshot_58.png\": [942, 113, 1001, 183],\n",
    "    \"Screenshot_59.png\": [396, 287, 465, 354],\n",
    "    \"Screenshot_6.png\": [122, 106, 844, 673],\n",
    "    \"Screenshot_60.png\": [540, 236, 653, 337],\n",
    "    \"Screenshot_61.png\": [896, 284, 1020, 437],\n",
    "    \"Screenshot_62.png\": [761, 318, 995, 591],\n",
    "    \"Screenshot_63.png\": [40, 203, 1397, 404],\n",
    "    \"Screenshot_64.png\": [72, 150, 1464, 348],\n",
    "    \"Screenshot_65.png\": [253, 221, 1333, 509],\n",
    "    \"Screenshot_66.png\": [633, 114, 699, 171],\n",
    "    \"Screenshot_68.png\": [659, 213, 715, 288],\n",
    "    \"Screenshot_69.png\": [13, 443, 77, 515],\n",
    "    \"Screenshot_7.png\": [246, 163, 1120, 555],\n",
    "    \"Screenshot_70.png\": [1053, 216, 1102, 273],\n",
    "    \"Screenshot_71.png\": [15, 154, 114, 231],\n",
    "    \"Screenshot_72.png\": [584, 157, 695, 248],\n",
    "    \"Screenshot_73.png\": [794, 489, 923, 691],\n",
    "    \"Screenshot_74.png\": [966, 237, 1005, 288],\n",
    "    \"Screenshot_75.png\": [1016, 278, 1074, 337],\n",
    "    \"Screenshot_76.png\": [575, 267, 633, 331],\n",
    "    \"Screenshot_77.png\": [1172, 329, 1255, 423],\n",
    "    \"Screenshot_78.png\": [1114, 350, 1226, 482],\n",
    "    \"Screenshot_79.png\": [127, 577, 306, 793],\n",
    "    \"Screenshot_8.png\": [17, 324, 104, 420],\n",
    "    \"Screenshot_80.png\": [968, 215, 1073, 305],\n",
    "    \"Screenshot_81.png\": [1059, 213, 1163, 312],\n",
    "    \"Screenshot_82.png\": [194, 198, 294, 553],\n",
    "    \"Screenshot_83.png\": [300, 473, 422, 594],\n",
    "    \"Screenshot_84.png\": [57, 398, 166, 508],\n",
    "    \"Screenshot_85.png\": [254, 284, 339, 369],\n",
    "    \"Screenshot_86.png\": [203, 505, 349, 637],\n",
    "    \"Screenshot_87.png\": [263, 254, 401, 373],\n",
    "    \"Screenshot_88.png\": [452, 309, 525, 383],\n",
    "    \"Screenshot_89.png\": [207, 220, 308, 302],\n",
    "    \"Screenshot_9.png\": [45, 149, 1140, 450],\n",
    "    \"Screenshot_90.png\": [139, 381, 232, 468],\n",
    "    \"Screenshot_91.png\": [630, 170, 760, 295],\n",
    "    \"Screenshot_92.png\": [971, 269, 1122, 409],\n",
    "    \"Screenshot_93.png\": [856, 427, 990, 594],\n",
    "    \"Screenshot_94.png\": [275, 311, 379, 457],\n",
    "    \"Screenshot_95.png\": [514, 169, 637, 291],\n",
    "    \"Screenshot_96.png\": [208, 158, 323, 296],\n",
    "    \"Screenshot_97.png\": [490, 173, 631, 290],\n",
    "    \"Screenshot_98.png\": [644, 266, 764, 414],\n",
    "    \"Screenshot_99.png\": [499, 109, 618, 208]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "data_dir = 'dataset'\n",
    "dataset = CustomDataset(data_dir, annotations_dict, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [img.to(device) for img, _ in batch]\n",
    "    annotations_list = [annotations for _, annotations in batch]\n",
    "\n",
    "    return images, annotations_list\n",
    "\n",
    "# Create the data loader with the custom collate function\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Load the pre-trained ResNet backbone\n",
    "backbone = torchvision.models.detection.backbone_utils.resnet_fpn_backbone('resnet50', pretrained=True)\n",
    "\n",
    "# Define the RPN anchor generator\n",
    "rpn_anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                       aspect_ratios=((0.5, 1.0, 2.0),) * 5)\n",
    "\n",
    "# Define the number of classes (including the background class)\n",
    "num_classes = 4 + 1  # +1 for the background class\n",
    "\n",
    "# Create the Faster R-CNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=num_classes,  # Number of object classes + background\n",
    "                   rpn_anchor_generator=rpn_anchor_generator)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)  # You can adjust the learning rate and momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'annotation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18000\\603293291.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations_list\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Iterate over your dataset batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\odyn\\anak2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\odyn\\anak2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\odyn\\anak2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python\\odyn\\anak2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18000\\3681781892.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'annotation'"
     ]
    }
   ],
   "source": [
    "num_epochs = 10  # Define the number of training epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, annotations_list in dataloader:  # Iterate over your dataset batches\n",
    "        images = [transform(img).to(device) for img in images]\n",
    "        \n",
    "        processed_annotations = []\n",
    "        for annotations_dict in annotations_list:\n",
    "            for bbox_coords in annotations_dict[\"bbox\"]:\n",
    "                x_min, y_min, x_max, y_max = bbox_coords\n",
    "                label = 1  # Assuming a single class for demonstration\n",
    "                \n",
    "                processed_annotation = {\n",
    "                    \"boxes\": torch.tensor([[x_min, y_min, x_max, y_max]], dtype=torch.float32),\n",
    "                    \"labels\": torch.tensor([label], dtype=torch.int64)\n",
    "                }\n",
    "                processed_annotations.append(processed_annotation)\n",
    "        \n",
    "        targets = processed_annotations\n",
    "        \n",
    "        # Forward pass and calculate losses\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'object_detection_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define a transformation for the test images (similar to the training transform)\n",
    "test_transform = T.Compose([\n",
    "    T.Resize((800, 800)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create a dataset for the test images\n",
    "test_dataset = CustomDataset('dataset', transform=test_transform)\n",
    "\n",
    "# Create a data loader for the test images\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# List to store results\n",
    "all_predictions = []\n",
    "\n",
    "# Iterate over test images and perform inference\n",
    "with torch.no_grad():\n",
    "    for images, _ in test_dataloader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        # Perform object detection\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Store the predictions\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "# Process the predictions as needed\n",
    "for predictions in all_predictions:\n",
    "    # Process the prediction to get the detected objects and their bounding boxes\n",
    "    # You can access predictions[0]['boxes'], predictions[0]['labels'], predictions[0]['scores']\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "\n",
    "    # Process and visualize the results\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        # Process the bounding box, label, and score as needed\n",
    "        print(\"Label:\", label, \"Score:\", score, \"Box:\", box)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
